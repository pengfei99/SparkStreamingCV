{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import io\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkStreamingComputerVision\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"liupengfei99/sparkstreamincv:latest\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\",os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\",os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.executorEnv.AWS_SESSION_TOKEN\",os.getenv(\"AWS_SESSION_TOKEN\")) \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Check the created cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up variables and dependent functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"pengfei\"\n",
    "image_input_folder_path = \"s3a://{}/tmp/sparkcv/input\".format(bucket_name)\n",
    "final_output_bucket_path = \"tmp/sparkcv/output/final\"\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "AWS_ACCESS_KEY_ID=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "SESSION_TOKEN=os.environ['AWS_SESSION_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model from mlflow\n",
    "def get_model_from_mlflow_registry(model_name,stage):\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = 'https://pengfei-mlflow-5806178270327072668-mlflow-ihm.kub.sspcloud.fr/'\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://minio.lab.sspcloud.fr\"\n",
    "    return mlflow.keras.load_model(model_uri=f\"models:/{model_name}/{stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deseriallize byte to opencv image format\n",
    "def convert_byte_to_nparr(img_byte):\n",
    "    np_array = cv.imdecode(np.asarray(bytearray(img_byte)), cv.IMREAD_COLOR)\n",
    "    return np_array\n",
    "\n",
    "# serialize opencv image format to byte\n",
    "def convert_nparr_to_byte(img_np_array):\n",
    "    success, img = cv.imencode('.png', img_np_array)\n",
    "    return img.tobytes()\n",
    "\n",
    "# save image byte to s3\n",
    "def write_img_byte_to_s3(s3_client, img_byte, bucket_name, img_path):\n",
    "    # set the path of where you want to put the object\n",
    "    img_object = s3_client.Object(bucket_name, img_path)\n",
    "    # set the content which you want to write\n",
    "    img_object.put(Body=img_byte)\n",
    "\n",
    "# column function for extract image name\n",
    "def extract_file_name(path):\n",
    "    return f.substring_index(path, \"/\", -1)\n",
    "\n",
    "# render image byte in jupyter\n",
    "def render_image(image_bytes_list):\n",
    "    for image_bytes in image_bytes_list:\n",
    "        image=Image.open(io.BytesIO(image_bytes))\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for step 2 (Extract multiple faces from the origin image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_extraction(image_name, raw_img_content):\n",
    "    haar_model_name = \"haarcascade_frontalface_default.xml\"\n",
    "    haar_model_path = \"{}{}\".format(\"/opt/conda/lib/python3.7/site-packages/cv2/data/\",haar_model_name)\n",
    "    img = cv.imdecode(np.asarray(bytearray(raw_img_content)), cv.IMREAD_COLOR)\n",
    "    img = cv.cvtColor(img, cv.IMREAD_GRAYSCALE)\n",
    "    face_model = cv.CascadeClassifier(haar_model_path)\n",
    "    faces = face_model.detectMultiScale(img, scaleFactor=1.1, minNeighbors=4)  # returns a list of (x,y,w,h) tuples\n",
    "\n",
    "    # Extract faces from the origin image\n",
    "    extracted_face_list = []\n",
    "    for i in range(len(faces)):\n",
    "        (x, y, w, h) = faces[i]\n",
    "        img_content = img[y:y + h, x:x + w]\n",
    "        img_content = cv.resize(img_content, (128, 128))\n",
    "        extracted_face_img_name = image_name[:-4] + \"_x\" + str(x) + \"_y\" + str(y) + \"_w\" + str(\n",
    "            w) + \"_h\" + str(h) + \".png\"\n",
    "        img_byte = convert_nparr_to_byte(img_content)\n",
    "\n",
    "        extracted_face_list.append((extracted_face_img_name, img_byte))\n",
    "    return extracted_face_list\n",
    "\n",
    "\n",
    "face_extraction_schema = ArrayType(StructType([\n",
    "    StructField(\"img_name\", StringType(), False),\n",
    "    StructField(\"img_content\", BinaryType(), False)\n",
    "]))\n",
    "\n",
    "Face_Extraction_UDF = f.udf(lambda image_name, raw_image_content: face_extraction(image_name, raw_image_content),\n",
    "                            face_extraction_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for step 3 (Predict if the extracted face wear a mask or not) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_mask_prediction(np_img_str):\n",
    "    vgg19_model_path = \"/home/jovyan/work\"\n",
    "    vgg19_model_name = \"masknet.h5\"\n",
    "    # read raw face image\n",
    "    np_arr_img = convert_byte_to_nparr(np_img_str)\n",
    "    img = np.reshape(np_arr_img, [1, 128, 128, 3])\n",
    "    img = img / 255.0\n",
    "    # fetch local model\n",
    "    # vgg19_model = tf.keras.models.load_model(\"{}/{}\".format(vgg19_model_path, vgg19_model_name))\n",
    "    \n",
    "    # fetch model from mlflow\n",
    "    vgg19_model=get_model_from_mlflow_registry(\"face-mask-detection\",\"Production\")\n",
    "    score = vgg19_model.predict(img)\n",
    "    if np.argmax(score) == 0:\n",
    "        res = True\n",
    "    else:\n",
    "        res = False\n",
    "    # print(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "Face_Mask_Prediction_UDF = f.udf(lambda face_image_content: face_mask_prediction(face_image_content), BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for step 4(Integrate the prediction result to the original image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_face_coordinate_of_origin_image(face_image_name):\n",
    "    x = face_image_name.split(\"_\")[1][1:]\n",
    "    y = face_image_name.split(\"_\")[2][1:]\n",
    "    w = face_image_name.split(\"_\")[3][1:]\n",
    "    h = face_image_name.split(\"_\")[4][1:].split('.')[0]\n",
    "    return int(x), int(y), int(w), int(h)\n",
    "\n",
    "\n",
    "def integrate_face_mask_prediction(origin_image_name, face_list, origin_image_content):\n",
    "    final_output_path = \"/tmp/sparkcv/output/final/{}\".format(origin_image_name)\n",
    "    buffer_img = cv.imdecode(np.asarray(bytearray(origin_image_content)), cv.IMREAD_COLOR)\n",
    "    for face in face_list:\n",
    "        face_image_name = face[0]\n",
    "        has_mask = face[1]\n",
    "        # set Label text\n",
    "        if has_mask:\n",
    "            mask_label = \"MASK\"\n",
    "        else:\n",
    "            mask_label = \"NO MASK\"\n",
    "        # Get the coordinate and size of face image\n",
    "        (x, y, w, h) = get_face_coordinate_of_origin_image(face_image_name)\n",
    "        # Set text color for mask label\n",
    "        mask_label_color = {\"MASK\": (0, 255, 0), \"NO MASK\": (0, 0, 255)}\n",
    "\n",
    "        # Insert mask label to image\n",
    "        buffer_img = cv.putText(buffer_img, mask_label, (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                mask_label_color[mask_label], 2)\n",
    "        # Insert a rectangle around the face\n",
    "        buffer_img = cv.rectangle(buffer_img, (x, y), (x + w, y + h), mask_label_color[mask_label], 1)\n",
    "    # serialize cv image to bytes\n",
    "    img_bytes=convert_nparr_to_byte(buffer_img)\n",
    "    # Save the image to s3\n",
    "    s3_client = boto3.resource('s3',endpoint_url=endpoint,aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                               aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "                               aws_session_token=SESSION_TOKEN,\n",
    "                               config=Config(signature_version='s3v4'),\n",
    "                               region_name='us-east-1')\n",
    "    output_bucket_key=\"{}/{}\".format(final_output_bucket_path,origin_image_name)\n",
    "    write_img_byte_to_s3(s3_client, img_bytes, bucket_name, output_bucket_key)\n",
    "    return img_bytes\n",
    "\n",
    "\n",
    "Integrate_Face_Mask_Prediction_UDF = f.udf(\n",
    "    lambda origin_img_name, face_list, origin_img_content: integrate_face_mask_prediction(origin_img_name, face_list,origin_img_content),BinaryType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main data pipeline for treating image\n",
    "To check if everyone wears a face mask or not in an image, we will follow the below steps:\n",
    "1. Read raw image from s3\n",
    "2. Detect faces from the raw image, output extracted faces as single images(opencv:haar-cascade)\n",
    "3. Use a pre-trained vgg19 model to check if a mask is worn\n",
    "4. Integrate prediction as tags on origin image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get raw image from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_schema = spark.read.format(\"binaryFile\").load(image_input_folder_path).schema\n",
    "raw_image_df = spark.read \\\n",
    "        .format(\"binaryFile\") \\\n",
    "        .schema(image_schema) \\\n",
    "        .option(\"maxFilesPerTrigger\", \"500\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.png\") \\\n",
    "        .load(image_input_folder_path)\n",
    "raw_image_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the origin image\n",
    "origin_col_name=\"content\"\n",
    "origin_image_list = raw_image_df.select(origin_col_name).toPandas()[origin_col_name]\n",
    "render_image(origin_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clean the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name_df = raw_image_df \\\n",
    "        .select(\"path\", \"content\") \\\n",
    "        .withColumn(\"origin_image_name\", extract_file_name(f.col(\"path\"))).drop(\"path\")\n",
    "image_name_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use udf Face_Extraction_UDF to extract faces\n",
    "detected_face_list_df = image_name_df.withColumn(\"detected_face_list\",Face_Extraction_UDF(\"origin_image_name\", \"content\"))\n",
    "detected_face_list_df.show()\n",
    "detected_face_list_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat the list column to multi rows\n",
    "detected_face_ob_df = detected_face_list_df.withColumn(\"extracted_face\",f.explode(f.col(\"detected_face_list\"))).drop(\"detected_face_list\")\n",
    "detected_face_ob_df.show()\n",
    "detected_face_ob_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat the struct column to primitive column\n",
    "detected_face_df = detected_face_ob_df.select(f.col(\"origin_image_name\"),f.col(\"content\"),\n",
    "                                                  f.col(\"extracted_face.img_name\").alias(\"extracted_face_image_name\"),\n",
    "                                                  f.col(\"extracted_face.img_content\").alias(\n",
    "                                                      \"extracted_face_image_content\"))\n",
    "detected_face_df.show()\n",
    "detected_face_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the extracted faces\n",
    "face_col_name=\"extracted_face_image_content\"\n",
    "face_image_list = detected_face_df.select(face_col_name).toPandas()[face_col_name]\n",
    "render_image(face_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 : Predict if the extracted faces wear masks or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask_df = detected_face_df.withColumn(\"with_mask\",Face_Mask_Prediction_UDF(\"extracted_face_image_content\")).cache()\n",
    "predicted_mask_df.show()\n",
    "predicted_mask_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Integrate the prediction result to the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the face name with the mask prediction result and group them by their origin \n",
    "grouped_face_df = predicted_mask_df.drop(\"extracted_face_image_content\").groupBy(\"origin_image_name\",\"content\").agg(f.collect_list(f.struct(\n",
    "                *[f.col(\"extracted_face_image_name\").alias(\"face_name\"), f.col(\"with_mask\").alias(\"with_mask\")]))\n",
    "            .alias(\"face_list\"))\n",
    "\n",
    "grouped_face_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate prediction and write to s3\n",
    "final_df = grouped_face_df.withColumn(\"marked_img_content\",Integrate_Face_Mask_Prediction_UDF(\"origin_image_name\", \"face_list\",\"content\"))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the marked_img_content\n",
    "col_name=\"marked_img_content\"\n",
    "image_list = final_df.select(col_name).toPandas()[col_name]\n",
    "render_image(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop sparksession\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dependencies\n",
    "pip install opencv-contrib-python\n",
    "pip install mlflow --user\n",
    "git clone https://github.com/pengfei99/SparkStreamingCV.git\n",
    "pip install tensorflow\n",
    "pip install --upgrade  protobuf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
