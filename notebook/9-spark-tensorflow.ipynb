{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark tensorflow\n",
    "\n",
    "Dans les tutoriels précédents nous avons travaillé avec des dataset textuel.\n",
    "\n",
    "Nous pouvons aussi utiliser les\n",
    "model de machine learning dans notre spark data pipeline.\n",
    "\n",
    "Dans ce tutoriel, nous allons apprendre comment utiliser spark pour traiter les images.\n",
    "Nous allons voire aussi comment utiliser les model de machine learning dans notre spark\n",
    "data pipeline. Notre data pipeline suivre les etapes suivants :\n",
    "1. Utiliser spark pour lire et nettoyer les images originaux,\n",
    "2. Utiliser haar-cascade model pour detect les visages et extraire chaque visage comme un\n",
    "image individuel\n",
    "3. Utiliser un model de classification pre-entrainer avec tensorflow pour verifier\n",
    "si le mask est bien porte ou pas\n",
    "4. Integrer les prediction de notre model sur les images originaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installer les dependencies\n",
    "\n",
    "Comme nous allons utilise des libraries pas standard, nous devons les installer dans notre spark\n",
    "driver. Ouvrir un terminal, et execute les command suivant.\n",
    "```shell\n",
    "pip install opencv-contrib-python\n",
    "pip install tensorflow\n",
    "pip install --upgrade  protobuf\n",
    "\n",
    "# vous pouvez verifier s'ils sont bien install en utilisant\n",
    "pip show <package-name>\n",
    "```\n",
    "Vous pouvez les installe directement via notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install opencv-contrib-python"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install --upgrade  protobuf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du context Spark\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fa270555850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkStreamingComputerVision\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .config(\"spark.kubernetes.executor.podTemplateFile\",\"/home/jovyan/work/SparkStreamingCV/k8s_pod_template/custom_python_dependencies.yaml\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que :\n",
    "* on a pris 4 executeurs et on surcharge spark pour, lors de shuffle ou repartition, que son niveau de parallelisme soit\n",
    "4 plutôt que 200\n",
    "* comme spark est un framework de calcul distribuer, les executor doit avoir les meme dependencies comme driver. on a\n",
    "plusieur solutions: 1.) Build un image specific et indiquer spark executor utilise ce image. 2.) Utilise un spark\n",
    "feature \"podTemplate\" pour installer les dependencies sur une image standard. Nous allons choisi la solution 2, car\n",
    "c'est plus souple et leger."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configurer variable\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image path\n",
    "image_input_folder_path = \"s3a://projet-spark-lab/diffusion/spark_cv/data/input/\"\n",
    "# model path\n",
    "face_mask_model_url=\"https://minio.lab.sspcloud.fr/projet-spark-lab/diffusion/spark_cv/models/masknet.h5\"\n",
    "\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "AWS_ACCESS_KEY_ID=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "SESSION_TOKEN=os.environ['AWS_SESSION_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure helper function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001B[K     |████████████████████████████████| 154 kB 5.2 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from graphframes) (1.18.1)\n",
      "Installing collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# deseriallize byte to opencv image format\n",
    "def convert_byte_to_nparr(img_byte):\n",
    "    np_array = cv.imdecode(np.asarray(bytearray(img_byte)), cv.IMREAD_COLOR)\n",
    "    return np_array\n",
    "\n",
    "# serialize opencv image format to byte\n",
    "def convert_nparr_to_byte(img_np_array):\n",
    "    success, img = cv.imencode('.png', img_np_array)\n",
    "    return img.tobytes()\n",
    "\n",
    "# column function for extract image name\n",
    "def extract_file_name(path):\n",
    "    return f.substring_index(path, \"/\", -1)\n",
    "\n",
    "# render image byte in jupyter\n",
    "def render_image(image_bytes_list):\n",
    "    for image_bytes in image_bytes_list:\n",
    "        image=Image.open(io.BytesIO(image_bytes))\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions pour etap 2 (Detecte les visages et extraire chaque visage comme un image individual)\n",
    "\n",
    "Vous pouvez remarque que, a la fin du block, on cree un spark udf(user define function). Les UDF\n",
    "permettent de créer une nouvelle colonne dans un dataframe qui sera le résultat d’un calcul\n",
    "pouvant utiliser les valeurs d’une (ou plusieurs) colonne(s) existante(s).\n",
    "\n",
    "Dans notre cas, notre udf prend un image et retourne une list d'objet(sous-image) qui contient tous les\n",
    "visages extraire d'image original. L'object a deux champs:\n",
    "1. Le nom en string qui contient la position de visage dans l'image original.\n",
    "2. Le contenu d'image en byte"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def face_extraction(image_name, raw_img_content):\n",
    "    haar_model_name = \"haarcascade_frontalface_default.xml\"\n",
    "    haar_model_path = \"{}{}\".format(\"/opt/conda/lib/python3.7/site-packages/cv2/data/\",haar_model_name)\n",
    "    img = cv.imdecode(np.asarray(bytearray(raw_img_content)), cv.IMREAD_COLOR)\n",
    "    img = cv.cvtColor(img, cv.IMREAD_GRAYSCALE)\n",
    "    face_model = cv.CascadeClassifier(haar_model_path)\n",
    "    faces = face_model.detectMultiScale(img, scaleFactor=1.1, minNeighbors=4)  # returns a list of (x,y,w,h) tuples\n",
    "\n",
    "    # Extract faces from the origin image\n",
    "    extracted_face_list = []\n",
    "    for i in range(len(faces)):\n",
    "        (x, y, w, h) = faces[i]\n",
    "        img_content = img[y:y + h, x:x + w]\n",
    "        img_content = cv.resize(img_content, (128, 128))\n",
    "        extracted_face_img_name = image_name[:-4] + \"_x\" + str(x) + \"_y\" + str(y) + \"_w\" + str(\n",
    "            w) + \"_h\" + str(h) + \".png\"\n",
    "        img_byte = convert_nparr_to_byte(img_content)\n",
    "\n",
    "        extracted_face_list.append((extracted_face_img_name, img_byte))\n",
    "    return extracted_face_list\n",
    "\n",
    "\n",
    "face_extraction_schema = ArrayType(StructType([\n",
    "    StructField(\"img_name\", StringType(), False),\n",
    "    StructField(\"img_content\", BinaryType(), False)\n",
    "]))\n",
    "\n",
    "Face_Extraction_UDF = f.udf(lambda image_name, raw_image_content: face_extraction(image_name, raw_image_content),\n",
    "                            face_extraction_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions pour etap 3 (predict si un mask est bien porte ou pas)\n",
    "\n",
    "Dans ce spark udf, il prend un image, si le model predit que le mask est bien porte, le udf\n",
    "retourne true, sinon il retourne false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_mask_prediction(np_img_str):\n",
    "    # read raw face image\n",
    "    np_arr_img = convert_byte_to_nparr(np_img_str)\n",
    "    img = np.reshape(np_arr_img, [1, 128, 128, 3])\n",
    "    img = img / 255.0\n",
    "    # fetch model from s3\n",
    "    model_path = get_file('masknet.h5', face_mask_model_url)\n",
    "    vgg19_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    score = vgg19_model.predict(img)\n",
    "    if np.argmax(score) == 0:\n",
    "        res = True\n",
    "    else:\n",
    "        res = False\n",
    "    # print(res)\n",
    "    return res\n",
    "\n",
    "Face_Mask_Prediction_UDF = f.udf(lambda face_image_content: face_mask_prediction(face_image_content), BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions pour etap 4 (Integre les resultats sur l'image original\n",
    "\n",
    "Pour que les resultats de notre prediction soit plus lisible, nous allons integres les resultats de\n",
    "prediction sur l'image original en utilisant les tags mask/no-mask."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|src|dst|action|\n",
      "+---+---+------+\n",
      "|  1|  2|  love|\n",
      "|  2|  1|  hate|\n",
      "|  2|  3|follow|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract face coordinate from the face image name\n",
    "def get_face_coordinate_of_origin_image(face_image_name):\n",
    "    x = face_image_name.split(\"_\")[1][1:]\n",
    "    y = face_image_name.split(\"_\")[2][1:]\n",
    "    w = face_image_name.split(\"_\")[3][1:]\n",
    "    h = face_image_name.split(\"_\")[4][1:].split('.')[0]\n",
    "    return int(x), int(y), int(w), int(h)\n",
    "\n",
    "\n",
    "def integrate_face_mask_prediction(origin_image_name, face_list, origin_image_content):\n",
    "    buffer_img = cv.imdecode(np.asarray(bytearray(origin_image_content)), cv.IMREAD_COLOR)\n",
    "    for face in face_list:\n",
    "        face_image_name = face[0]\n",
    "        has_mask = face[1]\n",
    "        # set Label text\n",
    "        if has_mask:\n",
    "            mask_label = \"MASK\"\n",
    "        else:\n",
    "            mask_label = \"NO MASK\"\n",
    "        # Get the coordinate and size of face image\n",
    "        (x, y, w, h) = get_face_coordinate_of_origin_image(face_image_name)\n",
    "        # Set text color for mask label\n",
    "        mask_label_color = {\"MASK\": (0, 255, 0), \"NO MASK\": (0, 0, 255)}\n",
    "\n",
    "        # Insert mask label to image\n",
    "        buffer_img = cv.putText(buffer_img, mask_label, (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                mask_label_color[mask_label], 2)\n",
    "        # Insert a rectangle around the face\n",
    "        buffer_img = cv.rectangle(buffer_img, (x, y), (x + w, y + h), mask_label_color[mask_label], 1)\n",
    "    # serialize cv image to bytes\n",
    "    img_bytes=convert_nparr_to_byte(buffer_img)\n",
    "    return img_bytes\n",
    "\n",
    "\n",
    "Integrate_Face_Mask_Prediction_UDF = f.udf(\n",
    "    lambda origin_img_name, face_list, origin_img_content: integrate_face_mask_prediction(origin_img_name, face_list,origin_img_content),BinaryType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le data pipeline principal pour traiter les images\n",
    "\n",
    "1. Utiliser spark pour lire et nettoyer les images originaux,\n",
    "2. Utiliser haar-cascade model pour detect les visages et extraire chaque visage comme un\n",
    "image individuel\n",
    "3. Utiliser un model de classification pre-entrainer avec tensorflow pour verifier\n",
    "si le mask est bien porte ou pas\n",
    "4. Integrer les prediction de notre model sur les images originaux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Etap 1: Lire l'image de s3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_schema = spark.read.format(\"binaryFile\").load(image_input_folder_path).schema\n",
    "raw_image_df = spark.read \\\n",
    "        .format(\"binaryFile\") \\\n",
    "        .option(\"maxFilesPerTrigger\", \"500\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.png\") \\\n",
    "        .schema(image_schema) \\\n",
    "        .load(image_input_folder_path)\n",
    "raw_image_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the origin image\n",
    "origin_col_name=\"content\"\n",
    "origin_image_list = raw_image_df.select(origin_col_name).toPandas()[origin_col_name]\n",
    "render_image(origin_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Nettoye le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "image_name_df = raw_image_df \\\n",
    "        .select(\"path\", \"content\") \\\n",
    "        .withColumn(\"origin_image_name\", extract_file_name(f.col(\"path\"))).drop(\"path\")\n",
    "image_name_df.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Etap 2: Extraire les visages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# use udf Face_Extraction_UDF to extract faces\n",
    "detected_face_list_df = image_name_df.withColumn(\"detected_face_list\",Face_Extraction_UDF(\"origin_image_name\", \"content\"))\n",
    "detected_face_list_df.show()\n",
    "detected_face_list_df.printSchema()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat the list column to multi rows\n",
    "detected_face_ob_df = detected_face_list_df.withColumn(\"extracted_face\",f.explode(f.col(\"detected_face_list\"))).drop(\"detected_face_list\")\n",
    "detected_face_ob_df.show()\n",
    "detected_face_ob_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# flat the struct column to primitive column\n",
    "detected_face_df = detected_face_ob_df.select(f.col(\"origin_image_name\"),f.col(\"content\"),\n",
    "                                                  f.col(\"extracted_face.img_name\").alias(\"extracted_face_image_name\"),\n",
    "                                                  f.col(\"extracted_face.img_content\").alias(\n",
    "                                                      \"extracted_face_image_content\"))\n",
    "detected_face_df.show()\n",
    "detected_face_df.printSchema()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the extracted faces\n",
    "face_col_name=\"extracted_face_image_content\"\n",
    "face_image_list = detected_face_df.select(face_col_name).toPandas()[face_col_name]\n",
    "render_image(face_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etap 3 : Donne le prediction de mask sur les visages extraire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask_df = detected_face_df.withColumn(\"with_mask\",Face_Mask_Prediction_UDF(\"extracted_face_image_content\")).cache()\n",
    "predicted_mask_df.show()\n",
    "predicted_mask_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Etap 4: Integrer les prediction de notre model sur les images originaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------+\n",
      "|id      |name                                  |\n",
      "+--------+--------------------------------------+\n",
      "|4898091 |FinancialTimes                        |\n",
      "|7540072 |neufmetres                            |\n",
      "|10575072|Webzine de la dracenie et du Var Est ن|\n",
      "|16683666|spectator                             |\n",
      "|17385313|Julien_W                              |\n",
      "|17437184|alphoenix                             |\n",
      "|17464719|PascalR                               |\n",
      "|17779850|Jennifer Ogor                         |\n",
      "|18976358|SylvainePascual                       |\n",
      "|19713578|chris dabin                           |\n",
      "+--------+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map the face name with the mask prediction result and group them by their origin\n",
    "grouped_face_df = predicted_mask_df.drop(\"extracted_face_image_content\").groupBy(\"origin_image_name\",\"content\").agg(f.collect_list(f.struct(\n",
    "                *[f.col(\"extracted_face_image_name\").alias(\"face_name\"), f.col(\"with_mask\").alias(\"with_mask\")]))\n",
    "            .alias(\"face_list\"))\n",
    "\n",
    "grouped_face_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# integre les predictions\n",
    "final_df = grouped_face_df.withColumn(\"marked_img_content\",Integrate_Face_Mask_Prediction_UDF(\"origin_image_name\", \"face_list\",\"content\"))\n",
    "final_df.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche les image avec les tags \"mask\" \"no_mask\"\n",
    "col_name=\"marked_img_content\"\n",
    "image_list = final_df.select(col_name).toPandas()[col_name]\n",
    "render_image(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---+--------------------+--------------------+\n",
      "|     src|               dst| nb|            hashtags|                  id|\n",
      "+--------+------------------+---+--------------------+--------------------+\n",
      "|15217683|793749212118867969|  1|                  []|[1380995523168120...|\n",
      "|15872615|         121468512|  1|                  []|[1380245558141591...|\n",
      "|16600674|         217473382|  6|                  []|[1380505509611069...|\n",
      "|17193568|         217473382|  1|          [immigrés]|[1380513459482337...|\n",
      "|17385313|         217473382|  6|                  []|[1380247279894986...|\n",
      "|17464719|        2515649016|  1|                  []|[1380436148057608...|\n",
      "|18629937|         121468512|  1|                  []|[1380492059690295...|\n",
      "|18969131|          20947741|  1|                  []|[1380468019009355...|\n",
      "|19377400|         112754792|  1|                  []|[1381174506446848...|\n",
      "|20064944|        1460135654|  1|                  []|[1380449540738850...|\n",
      "|20181221|          46375782|  1|                  []|[1381281068037369...|\n",
      "|20181221|        1325327106|  1|                  []|[1381281068037369...|\n",
      "|20773587|          26073581|  2|                  []|[1381742535253573...|\n",
      "|20773587|         305783924|  2|                  []|[1381742535253573...|\n",
      "|21040954|          15618138|  1|                  []|[1380511284807012...|\n",
      "|22662958|          97514372|  1|                  []|[1382931299300704...|\n",
      "|23931979|         103918784|  1|                  []|[1380485755651362...|\n",
      "|27016118|983334079981654016|  1|                  []|[1381295795186585...|\n",
      "|28111905|        1187428428|  1|[JDD, Arras, Gran...|[1381340432462970...|\n",
      "|36017082|793749212118867969|  1|                  []|[1380866702695669...|\n",
      "+--------+------------------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stop sparksession\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}